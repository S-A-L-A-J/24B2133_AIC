Our aim is to read a PDF file and use sections or chunks which contain some kind of related information. After breaking the pdf into chunks we in a vector database, when a question is asked, the model will retrieve the most relevant chunk to the question and then sends the whole chunk as an input to a open source LLM. We will give a call to the LLM which will explain the chunk and our function will return the LLM's answer.

To accomplish this, we will start from the basics, learning how a PDF works and how to retreive data from a pdf. 
A pdf file is structured as a set of objects and they have some attributes or properties like text boxes, fonts, styles, positions, images, annotations, etc. 
To read our given PDF, i used the PyMuPDF library which loads each page of the pdf in a structured layout. When we use the library we can get outptut in many different ways, like plain text, list of text blocks with their coordinates, full structure or HTML. 

After reading the Pdf, we need to divide it into sections which contain some meaning or are related to each other. This method is called semantic chunking, we divide the text into chunks and each chunk contains some semantic meaning. Now, each of the chunk is embedded with a embedding vector, i.e. a vector with numbers represented in a high dimensional space. This is the same embedding which occurs in the transformer which we understood for the Q1. The encoder in a transformer uses similar method to encode data into vectors and vectors with similar directions portray similar concepts or ideas. This is very useful because instead of searching via keywords, we can directly search for a vectorial direction in the high dimensional space and the closest vector we can find to it, will be our selected chunk. This is exactly what we are doing while selecting the most relevant chunk to use as the input for the LLM api call. for this step i am using the all-MiniLM-L6-v2 model.

For finding the most relevant chunk as i explained above, we use FAISS. It exactly does what i explained above and i was able to visualize this thanks to 3B1B's video on transformers. So what FAISS does is, it constructs a vector storage where all the embedded vectors are stored. when a question is asked, it creates a similar embedding and creates a query vector, now it tries to find the closest k vectors to the query vector by calculating the L2 eucleadian distance. There are a few types of indexing methods too. 

After generating the query vector and finding the closest and most relevant chunks, we send these things in a very specific format to an LLM api, i will use the GROQ api becuase it is cloud based and quite fast. So when we send the query and the relevant chunks, the LLM will read the relevant chunks and construct an answer for the query from it. I think this is the exact thing which happens when we attach a pdf or slides to chatgpt and ask questions from it. The bot wont go through the pdf everytime obviously, it stores the whole data into high dimensional vectors and uses only the data which is relevant to it. 